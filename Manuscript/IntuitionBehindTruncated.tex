\documentclass[a4paper,11pt]{article}
\usepackage{latexsym,amssymb,enumerate,amsmath,epsfig,amsthm,dsfont,bm}
\usepackage[margin=1in]{geometry}
\usepackage{setspace,color}
\usepackage{tikz}
\usetikzlibrary{intersections,positioning,calc}
\usepackage{floatrow}
\usepackage{graphicx,subfigure}
\usepackage[ruled]{algorithm2e}
\usepackage{epstopdf}
\usepackage{tcolorbox}
\usepackage[multidot]{grffile}
\usepackage{comment}
\usepackage{multirow}
\newcommand{\x}{\mathbf{x}}
\newcommand{\p}{\mathbf{p}}
\newcommand{\n}{\mathbf{n}}
\newcommand{\m}{\mathbf{m}}
\newcommand{\tEet}{\widetilde{E}_{2,\varepsilon}}
\newcommand{\tEeo}{\widetilde{E}_{1,\varepsilon}}
\newcommand{\de}{\delta_{\varepsilon}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\bflambda}{\bm{\lambda}}
\newcommand{\maF}{\mathcal{F}}
\newcommand\setItemnumber[1]{\setcounter{enumi}{\numexpr#1-1\relax}}
\newcommand{\reminder}[1]{\textcolor{red}{[[#1]]}}
\newcommand{\reminderr}[1]{\textcolor{blue}{[[#1]]}}

\newtheorem{remark}{Remark}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{thm}{Theorem}[section]
\newtheorem{prop}{Proposition}[section]
\newtheorem{obser}{Observation}[section]
\newtheorem{corollary}{Corollary}[section]

\newcommand{\cfbox}[2]{%
    \colorlet{currentcolor}{.}%
    {\color{#1}%
    \fbox{\color{currentcolor}#2}}%
}

\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}

%\doublespacing

\begin{document}

\title{Why Using Truncated Estimator Technique}
\author{Roy}

\maketitle
In this report, I briefly present the intuition behind using the truncated estimator technique in Silverman's paper~\cite{mack1982weak}. Their objective is  to prove that, given the data $\{(X_i,Y_i)\}_{i=1}^{n}$,
\begin{align}
	\sup_{x}|h_n(x)-Eh_n(x)|\to 0,~\text{a.s.}
\end{align}
where $h_n$ is a kernel estimator of the underlying true function $h:\mathbb{R}\to\mathbb{R}$ using the $n$ data points. Notice that $(X_i,Y_i)$ are i.i.d. random vectors of a relatively general distribution: They only require that $Y$ have bounded moments up to certain order. And it is the \textbf{truncated estimator technique} that allows this generalization, rather than just Gaussian.

The intuition behind this genius technique follows from three facts:
\begin{enumerate}
\item For a rather general random variable $Y$, it's empirical distribution should approach to it's distribution function when the size of the data increases. And the empirical distribution is a summation of i.i.d. random variables. 
\item Certain summation of i.i.d. random variables can be well approximated by summation of i.i.d. Gaussian random variables~\cite{tusnady1977remark}.
\item The truncated estimator $h_n^B$ should be close to $h_n$ when $B\to\infty$.	
\end{enumerate}
Notice that both 1 and 3 can be guaranteed with very high accuracy whenever the data size increases. In 2, the probability of the estimating error greater than a threshold does not depend on the data size; however, this threshold decreases as $n$ increases. \textit{Therefore, all the three items above depend on the sample size, and as it increases, the approximation error is controlled with high probability.}\\[10pt]


In Namjoon's proof: The key idea is \textit{Markov inequality}, and the probability of the error bound does not improve when the sample size is increased. The fundamental reason is that, the \textit{Markov inequality does not include the sample size information}. This explains why the upper-bound does not improve as the data size increases.

  






\bibliographystyle{unsrt}
\bibliography{Literature.bib}
\end{document}
